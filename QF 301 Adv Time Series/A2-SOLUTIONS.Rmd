---
output:
  html_document:
    df_print: paged
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```



# QF301.  Homework #2.


## `r format(Sys.time(), "%Y-%m-%d")`


I pledge on my honor that I have not given or received any unauthorized assistance on this assignment/examination. I further pledge that I have not copied any material from a book, article, the Internet or any other source except where I have expressly cited the source.

By filling out the following fields, you are signing this pledge.  No assignment will get credit without being pledged.

Name:

CWID:

Date:


# Instructions
In this assignment, you should use R markdown to answer the questions below. Simply type your R code into embedded chunks as shown above.
When you have completed the assignment, knit the document into a PDF file, and upload both the .pdf and .Rmd files to Canvas.
```{r}
CWID = -1 #Place here your Campus wide ID number, this will personalize
#your results, but still maintain the reproducible nature of using seeds.
#If you ever need to reset the seed in this assignment, use this as your seed
#Papers that use -1 as this CWID variable will earn 0's so make sure you change
#this value before you submit your work.
personal = CWID %% 10000
set.seed(personal)#You can reset the seed at any time in your code,
#but please always set it to this seed.
```
# Question 1 (20pt)

## Question 1.1
Use the quantmod package to obtain the daily adjusted close prices 2 different stocks.  You should have at least two years of data for both assets.  You should inspect the dates for your data to make sure you are including everything appropriately.  Create a data frame of the daily log returns both both stocks.  Print the first and last 6 lines of your data frame.

## \textcolor{red}{Solution:} 
```{r}
library(quantmod)

getSymbols(c("IBM","MSFT"),from="2018-01-01",to="2020-12-31") #IBM and Microsoft chosen at random

rIBM = dailyReturn(IBM$IBM.Adjusted,type="log")
rMSFT = dailyReturn(MSFT$MSFT.Adjusted,type="log")

df = data.frame(rIBM,rMSFT)
head(df)
tail(df)
```

## Question 1.2 
List the names of the variables in the data set.

## \textcolor{red}{Solution:} 
```{r}
colnames(df)

#Not necessary to do this, but to improve variable names:
colnames(df) = c("IBM","MSFT")
colnames(df)
```


## Question 1.3 
As the date will be unimportant, remove that field from your data frame

## \textcolor{red}{Solution:} 
```{r}
rownames(df) = NULL #Other approaches are valid as well
head(df)
tail(df)
```

## Question 1.4 
What is the mean and standard deviation of each variable? Create a simple table of the means and standard deviations.

## \textcolor{red}{Solution:} 
```{r}
sapply(df,mean)
sapply(df,sd)

dt = data.frame(sapply(df,mean),sapply(df,sd))
dt
```



## Question 1.5  
Regress one of your stock returns as a function of the other (simultaneous data). This should be of the form $r_1 = \beta_0 + \beta_1 r_2$. (No train/test split is required here.)

## \textcolor{red}{Solution:} 
```{r}
linear_regression = glm(IBM ~ MSFT,data=df)
summary(linear_regression)
```





# Question 2 (40pt)


## Question 2.1
Using the data set that you loaded for the first problem, choose one of your stocks, and create a data frame consisting of lagged returns going up to 5 days back.

## \textcolor{red}{Solution:} 
```{r}
#Choose IBM arbitrarily
r0 = as.numeric(rIBM)[-(1:5)]
r1 = as.numeric(lag(rIBM,k=1))[-(1:5)]
r2 = as.numeric(lag(rIBM,k=2))[-(1:5)]
r3 = as.numeric(lag(rIBM,k=3))[-(1:5)]
r4 = as.numeric(lag(rIBM,k=4))[-(1:5)]
r5 = as.numeric(lag(rIBM,k=5))[-(1:5)]

dfIBM = data.frame(r0,r1,r2,r3,r4,r5)
head(dfIBM)
```

## Question 2.2 
Split your data into a training set and a testing set (50% in each set). 
Create an AR(1) model for your stock returns. Provide the test mean squared error.

## \textcolor{red}{Solution:}     
```{r}
train = sample(length(r0),length(r0)/2,replace=FALSE)

ar1 = glm(r0 ~ r1 , data=dfIBM , subset=train)
summary(ar1)

pred=predict(ar1,dfIBM[-train,])
MSE = mean((pred-dfIBM$r0[-train])^2) #Test MSE
MSE #Test mean squared error
```

## Question 2.3
Evaluate if your AR(1) model is weakly stationary or unit-root nonstationary.

## \textcolor{red}{Solution:} 

```{r}
summary(ar1)$coefficients
b0 = summary(ar1)$coefficients[1,1]
b1 = summary(ar1)$coefficients[2,1]
b1.se = summary(ar1)$coefficients[2,2]
abs(b1) < 1 #If true then suspect weakly stationary

#Test for unit-root nonstationary
t = (b1 - 1)/b1.se
t
pnorm(t) #Large enough data, treat as normal
#If small enough then reject the null hypothesis
```

### (a) If your model is weakly stationary, provide the (long-run) average returns and autocovariances for your model.  How do these compare with the empirical values? If your model is (unit-root) nonstationary, please interpret your model.  Give as much detail as possible.

### \textcolor{red}{Solution:} 
This model is weakly stationary based on the conclusions above (|b1| < 1 and the Dickey-Fuller Test).
```{r}
# Average and autocovariances from the model
mu = b0/(1-b1)
mu
gamma0 = MSE/(1-b1^2)
gamma1 = b1*gamma0
gamma2 = b1*gamma1
gamma3 = b1*gamma2
gamma4 = b1*gamma3
gamma5 = b1*gamma4
c(gamma0,gamma1,gamma2,gamma3,gamma4,gamma5)

# vs. Empirical average and autocovariances
mean(r0) - mu
cov(r0,r0) - gamma0
cov(r0,r1) - gamma1
cov(r0,r2) - gamma2
cov(r0,r3) - gamma3
cov(r0,r4) - gamma4
cov(r0,r5) - gamma5
```

### (b) Provide the formulas for the 1- and 2- step ahead forecasts. What are the variances for these forecast errors?

### \textcolor{red}{Solution:} 
From class, the formulas are given by the below formulas where the constants are provided from the above trained AR(1) model.
$$
\hat{r}_t(1) = \beta_0 + \beta_1 r_t \\
\hat{r}_t(2) = \beta_0 + \beta_1 \hat{r}_t(1) \\
\operatorname{Var}(r_{t+1} - \hat{r}_t(1)) = \sigma_{\epsilon}^2 \\
\operatorname{Var}(r_{t+1} - \hat{r}_t(1)) = (1+\beta_1^2)\sigma_{\epsilon}^2
$$

### \textcolor{red}{Solution:} 

## Question 2.3
Using the same train/test split as in Question 2.2. 
Create an AR(5) model for your stock returns. Provide the test mean squared error.

## \textcolor{red}{Solution:} 
```{r}
ar5 = glm(r0 ~ . , data=dfIBM , subset=train)
summary(ar5)

pred=predict(ar5,dfIBM[-train,])
MSE = mean((pred-dfIBM$r0[-train])^2) #Test MSE
MSE #Test mean squared error
```

## Question 2.4
Using the same train/test split as in Question 2.2.
Using Ridge regression, produce an AR(5) linear regression.  Find the optimal tuning parameter for this regression.  What is the test mean squared error?

## \textcolor{red}{Solution:} 
```{r}
library(glmnet)

x=model.matrix(r0~.,dfIBM)[,-1]
y=dfIBM$r0

lambda=10^seq(-2,4,by=.01)
ridge.mod=glmnet(x[train,],y[train],alpha=0,lambda=lambda) #Ridge is with alpha=0
cv.out=cv.glmnet(x[train,],y[train],alpha=0,lambda=lambda) 
plot(cv.out)
ridge.bestlam=cv.out$lambda.min
ridge.bestlam

bestridge.pred=predict(ridge.mod,s=ridge.bestlam,newx=x[-train,])
mean((bestridge.pred-y[-train])^2)
```

## Question 2.5
Using the same train/test split as in Question 2.2.
Using LASSO regression, produce an AR(5) linear regression.  Find the optimal tuning parameter for this regression.  What is the test mean squared error?

## \textcolor{red}{Solution:} 
```{r}
lasso.mod=glmnet(x[train,],y[train],alpha=1,lambda=lambda) #LASSO is with alpha=1
cv.out=cv.glmnet(x[train,],y[train],alpha=1,lambda=lambda)
plot(cv.out)
lasso.bestlam=cv.out$lambda.min
lasso.bestlam

bestlasso.pred=predict(lasso.mod,s=lasso.bestlam,newx=x[-train,])
mean((bestlasso.pred-y[-train])^2)
```

## Question 2.6
Briefly (1 paragraph or less) compare the coefficients for the 3 AR(5) regressions computed.

## \textcolor{red}{Solution:} 
```{r}
summary(ar5)$coefficients

ridgecoef=glmnet(x[train,],y[train],alpha=0,lambda=lambda) #Can run without [train,] but fits as if all data is available
predict(ridgecoef,type="coefficients",s=ridge.bestlam)

lassocoef=glmnet(x[train,],y[train],alpha=1,lambda=lambda) #Can run without [train,] but fits as if all data is available
predict(lassocoef,type="coefficients",s=lasso.bestlam)
```
The AR(5) regression finds low predictive power by the slopes of all lags being non-zero.
The Ridge regression finds low predictive power (less than AR(5)) by the slopes of all lags being non-zero.
The LASSO regression finds that no lags are predictive for the current returns.



# Question 3 (15pt)

## Question 3.1
Write a function that works in R to gives you the parameters from a linear regression on a data set of $n$ predictors.  You can assume all the predictors and the prediction is numeric.  Include in the output the standard error of your variables.  You cannot use the lm command in this function or any of the other built in regression models.  

## \textcolor{red}{Solution:} 
```{r}
linear_regression = function(X,y) {
  beta = solve(t(X) %*% X) %*% t(X) %*% y
  sig2 = mean((y-X %*% beta)^2) #Mean squared error
  se = sig2*solve(t(X) %*% X) #Standard error (including covariances)
  se = diag(se) #Just pull of the standard errors
  OUTPUT = list(beta , se)
  names(OUTPUT) = c("beta","SE")
  return(OUTPUT)
}
```

## Question 3.2
Compare the output of your function to that of the lm command in R.

## \textcolor{red}{Solution:} 
```{r}
ar5 = lm(r0 ~ . , data=dfIBM)
summary(ar5)$coefficients

ones = rep(1,length(r0))
X = matrix(data=c(ones,r1,r2,r3,r4,r5) , nrow=length(r0),ncol=6)
my_ar5 = linear_regression(X,r0)
my_ar5$beta
my_ar5$SE
```
The beta values coincide exactly with that found with the lm command.
The computation of standard errors in my implementation is different from that done by R.



